## Hadoop has three components that were specifically designed to work on big data

### Storage Unit **HDFS** (Hadoop Distributed FS)

_Storing in one device is not feasible_

- [x] HDFS splits data into multiple blocks, then stored on several data nodes in the cluster. `128 MB each Block`.

- [x] HDFS makes copies of data and store it across multiple systems. -> `Replication Method`

Data is not lost at any cost; even when one DataNode crushes. -> `fault-tolerant`

### MapReduce

_Traditional data processing methods use single computing device._
`MapReduce` splits data into parts and process separately on different data nodes. Individual results are aggregrated together.

### YARN

_Resource Manager, Node Manager, Application Master, Containers_

- [ ] Application Master to request container from Node Manager.
- [ ] Once Node Manager gets resources -> sends to Resource Manager.

> YARN processes job requests & manage cluster resources.

## There are other components in Hadoop eco system

_Hive/Pig/Apache Spark/Flume/Sqoop_
